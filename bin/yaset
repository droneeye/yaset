#!/usr/bin/env python
import argparse
import configparser
import importlib
import logging
import os
import shutil
import sys
import time

from yaset.data.reader import TrainData, TestData
from yaset.nn.train import train_model, apply_model
from yaset.tools import ensure_dir

OPT_ALGO = ["sgd", "adam"]

MODEL_PARAMETERS = {
    "bilstm-char-crf": {
        "int_parameters": ["hidden_layer_size"],
        "float_parameters": ["dropout_rate"],
        "string_parameters": list(),
        "boolean_parameters": list(),
        "cond_parameters": {
            "use_char_embeddings": {
                "int_parameters": ["char_hidden_layer_size", "char_embedding_size"],
                "float_parameters": list(),
                "string_parameters": list(),
                "boolean_parameters": list(),
            }
        }
    }
}

GENERAL_PARAMETERS = {
    "int_parameters": ["max_iterations", "patience", "cpu_cores", "batch_size"],
    "float_parameters": ["opt_lr"],
    "string_parameters": ["model_type", "dev_metric", 'opt_algo'],
    "boolean_parameters": ["trainable_word_embeddings", "store_matrices_on_gpu"],
    "cond_parameters": {
        "opt_gc_use": {
            "int_parameters": list(),
            "float_parameters": ["opt_gs_val"],
            "string_parameters": list(),
            "boolean_parameters": list(),
        }
    }
}


def extract_params(configuration, section, params):
    """
    Fetch parameters values from a specific section of the configuration file
    :param configuration:
    :param section:
    :param params:
    :return:
    """

    parameters = dict()

    # HELPERS
    # ---------------------------------------------------------------

    def add_param(k, v):
        parameters[k] = v

    def get_int_param(k):
        return configuration.getint(section, k)

    def get_float_param(k):
        return configuration.getfloat(section, k)

    def get_string_param(k):
        return configuration.get(section, k)

    def get_boolean_param(k):
        return configuration.getboolean(section, k)

    # ---------------------------------------------------------------

    for int_param in params["int_parameters"]:
        add_param(int_param, get_int_param(int_param))

    for float_param in params["float_parameters"]:
        add_param(float_param, get_float_param(float_param))

    for string_param in params["string_parameters"]:
        add_param(string_param, get_string_param(string_param))

    for boolean_param in params["boolean_parameters"]:
        add_param(boolean_param, get_boolean_param(boolean_param))

    for cond_param in params["cond_parameters"]:

        add_param(cond_param, get_boolean_param(cond_param))

        if get_boolean_param(cond_param):

            for int_param in params["cond_parameters"][cond_param]["int_parameters"]:
                add_param(int_param, get_int_param(int_param))

            for float_param in params["cond_parameters"][cond_param]["float_parameters"]:
                add_param(float_param, get_float_param(float_param))

            for string_param in params["cond_parameters"][cond_param]["string_parameters"]:
                add_param(string_param, get_string_param(string_param))

            for boolean_param in params["cond_parameters"][cond_param]["boolean_parameters"]:
                add_param(boolean_param, get_boolean_param(boolean_param))

    return parameters


def get_training_parameters(configuration):
    """
    Fetch training parameters from a configparser object
    :param configuration: configparser object
    :return: a dictionary of parameters
    """

    final_parameters = dict()

    # Fetching general training parameters

    general_parameters = extract_params(configuration, "training", GENERAL_PARAMETERS)

    if general_parameters.get("model_type") not in MODEL_PARAMETERS:
        raise Exception("The neural network model you specified does not exist")

    model_parameters = extract_params(configuration,
                                      general_parameters.get("model_type"),
                                      MODEL_PARAMETERS[general_parameters.get("model_type")])

    for k, v in general_parameters.items():
        final_parameters[k] = v

    for k, v in model_parameters.items():
        final_parameters[k] = v

    return final_parameters


def parse_feature_columns(value):
    """
    Parse feature parameter
    :param value: feature value (str)
    :return:
    """

    raw_parts = value.split(",")
    raw_parts = [item.strip(" ") for item in raw_parts]

    final_list = [int(item) for item in raw_parts]

    return final_list


def log_message(message):

    logging.info("{} {} {}".format(
        "=" * 10,
        message,
        "=" * (70 - len(message) - 2 - 10)
    ))


if __name__ == "__main__":

    parser = argparse.ArgumentParser()

    parser.add_argument("--debug", action='store_true')

    subparsers = parser.add_subparsers(title="Sub-commands", description="Valid sub-commands",
                                       help="Valid sub-commands", dest="subparser_name")

    # 'Learn' subparser used to learn a new model
    parser_learn = subparsers.add_parser('LEARN', help="Learn model on train data")
    parser_learn.add_argument("--config", help="Configuration file (.ini format)", dest="config", type=str,
                              required=True)

    # 'Apply' subparser used to apply a pretrained model
    parser_test = subparsers.add_parser('APPLY', help="Apply model on test data")
    parser_test.add_argument("--model_path", help="Path to the model", dest="model_path", type=str, required=True)
    parser_test.add_argument("--input_file", help="Path to the tabulated test file", dest="input_file", required=True)
    parser_test.add_argument("--working_dir", help="Path where a working directory will be created", dest="working_dir",
                             required=True)

    args = parser.parse_args()

    # Timestamp for directory naming
    timestamp = time.strftime("%Y%m%d-%H%M%S")

    # LOGGING
    # ===============================================================
    # Logging to stdout

    log = logging.getLogger('')
    log_format = logging.Formatter("%(asctime)s %(levelname)s %(message)s")

    # Setting debug level
    if args.debug:
        log.setLevel(logging.DEBUG)
    else:
        log.setLevel(logging.INFO)

    # Toning down the verbosity of gensim (in both NORMAL and DEBUG modes)
    logging.getLogger('gensim').setLevel(logging.WARNING)

    # Adding a stdout handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(log_format)
    log.addHandler(ch)

    if args.subparser_name == "LEARN":

        # Check if config file does exist
        if not os.path.isfile(os.path.abspath(args.config)):
            raise FileNotFoundError("The configuration file you specified does not exist: {}".format(
                os.path.abspath(args.config)
            ))

        # Creating a configparser parser and parsing configuration
        parsed_configuration = configparser.ConfigParser(allow_no_value=True)
        parsed_configuration.read(os.path.abspath(args.config))

        # Checking if the top working dir specified in the configuration exists
        if not os.path.isdir(os.path.abspath(parsed_configuration["data"]["top_working_dir"])):
            raise NotADirectoryError("The top working directory you specified does not exist: {}".format(
                os.path.abspath(parsed_configuration["data"]["top_working_dir"])
            ))

        # Creating the current working directory based on the top working directory
        current_working_directory = os.path.join(
            os.path.abspath(parsed_configuration["data"]["top_working_dir"]),
            "yaset-learn-{}".format(timestamp)
        )
        ensure_dir(current_working_directory)

        # Setting up a log file and adding a new handler to the logger
        log_file = os.path.join(current_working_directory, "{}.log".format(
            "yaset-learn-{}".format(timestamp)
        ))

        fh = logging.FileHandler(log_file, encoding="UTF-8")
        fh.setFormatter(log_format)
        log.addHandler(fh)

        # Setting some variables
        train_file_path = os.path.abspath(parsed_configuration["data"]["train_file_path"])

        # -----------------------------------------------------------
        # DEV FILE

        dev_file_path = None
        dev_ratio = None
        dev_random_seed = None

        # If there is a 'dev' file provided, check if the file does exist
        if parsed_configuration.getboolean("data", "use_dev_file"):
            dev_file_path = os.path.abspath(parsed_configuration["data"]["dev_file_path"])

            if not os.path.isfile(dev_file_path):
                raise FileNotFoundError("The 'dev' file you specified doesn't exist: {}".format(
                    dev_file_path
                ))

        # If no 'dev' file is provided, fetch the train/dev ratio and check is it is between 0 and 1
        if not dev_file_path:
            dev_ratio = float(parsed_configuration["data"]["dev_ratio"])

            if dev_ratio <= 0 or dev_ratio >= 1:
                raise Exception("The 'dev' ratio must be between 0 and 1 (current ratio: {})".format(dev_ratio))

        # -----------------------------------------------------------
        # FEATURES

        feature_columns = list()

        if parsed_configuration.getboolean("data", "use_features"):
            feature_columns = parse_feature_columns(parsed_configuration["data"]["feature_columns"])

        # -----------------------------------------------------------
        # LOADING AND CHECKING DATA FILES

        log_message("BEGIN - LOADING AND CHECKING DATA FILES")

        data_lower_input = parsed_configuration.getboolean("data", "lower_input")
        data_replace_digits = parsed_configuration.getboolean("data", "replace_digits")

        # Creating data object
        data = TrainData(train_file_path, dev_data_file=dev_file_path, working_dir=current_working_directory,
                         dev_ratio=dev_ratio, feature_columns=feature_columns, lower_input=data_lower_input,
                         replace_digits=data_replace_digits)

        # Checking file format
        data.check_input_files()

        log_message("END - LOADING AND CHECKING DATA FILES")

        # -----------------------------------------------------------
        # EMBEDDING LOADING AND PROCESSING

        log_message("BEGIN - EMBEDDING LOADING AND PREPROCESSING")

        embedding_model_type = parsed_configuration["data"]["embedding_model_type"]

        if embedding_model_type is not "random":
            # Case where the model type is not random

            logging.info("Model type: {}".format(embedding_model_type))

            # Checking if the embedding model path does exist
            embedding_file_path = os.path.abspath(parsed_configuration["data"]["embedding_model_path"])

            if not os.path.isfile(embedding_file_path):
                raise FileNotFoundError("The embedding file you specified doesn't exist: {}".format(
                    embedding_file_path
                ))

            logging.info("File path: {}".format(embedding_file_path))

            embedding_map_unknown = parsed_configuration.getboolean("data", "map_unknown_token")
            embedding_unknown_item_id = None
            if embedding_map_unknown:
                embedding_unknown_item_id = parsed_configuration.get("data", "unknown_token_id")

            # Dynamic loading of embedding module. Allow to write custom modules for specific model formats.
            logging.info("Creating embedding object")
            embedding_module = importlib.import_module("yaset.embed.{}".format(embedding_model_type))
            embedding_class = getattr(embedding_module, "{}Embeddings".format(embedding_model_type.title()))
            embedding_object = embedding_class(embedding_file_path, embedding_unknown_item_id)

            # Loading embedding matrix into embedding object
            logging.info("Loading matrix")
            embedding_object.load_embedding()

            if not embedding_map_unknown:
                # Building unknown token vector based on embedding matrix
                logging.info("Building unknown token vector")
                embedding_unknown_item_id = embedding_object.build_unknown_token()
            else:
                logging.info("Unknown token vector already exists, skipping building new one (id={})".format(
                    embedding_unknown_item_id
                ))

        else:
            # Random embedding will be supported in a later release
            raise Exception("Random embeddings are not supported yet")

        log_message("END - EMBEDDING LOADING AND PREPROCESSING")

        log_message("BEGIN - CREATING TFRECORDS FILES")

        if parsed_configuration.getboolean("data", "dev_use_random_seed") and \
                not parsed_configuration.getboolean("data", "use_dev_file"):
            dev_random_seed = parsed_configuration.getint("data", "dev_random_seed")

        data.create_tfrecords_files(embedding_object, random_seed=dev_random_seed)

        log_message("END - CREATING TFRECORDS FILES")

        log_message("BEGIN - LEARNING MODEL")

        train_config = get_training_parameters(parsed_configuration)

        logging.debug("Current training parameters")
        for k, v in train_config.items():
            logging.debug("* {} = {}".format(k, v))

        train_model(current_working_directory, embedding_object, data, train_config)

        target_model_configuration_path = os.path.join(os.path.abspath(current_working_directory), "config.ini")
        shutil.copy(os.path.abspath(args.config), target_model_configuration_path)

        log_message("END - LEARNING MODEL")

    elif args.subparser_name == "APPLY":

        model_path = os.path.abspath(args.model_path)
        input_file = os.path.abspath(args.input_file)
        working_dir = os.path.abspath(args.working_dir)

        if not os.path.isdir(model_path):
            raise NotADirectoryError("The model path you specified does not exist: {}".format(model_path))

        if not os.path.isfile(input_file):
            raise FileNotFoundError("The input file you specified does not exist: {}".format(input_file))

        if not os.path.isdir(working_dir):
            raise NotADirectoryError("The working directory you specified does not exist: {}".format(working_dir))

        current_working_directory = os.path.join(working_dir, "yaset-apply-{}".format(timestamp))
        ensure_dir(current_working_directory)

        # Setting up a log file and adding a new handler to the logger
        log_file = os.path.join(current_working_directory, "{}.log".format(
            "yaset-apply-{}".format(timestamp)
        ))

        fh = logging.FileHandler(log_file, encoding="UTF-8")
        fh.setFormatter(log_format)
        log.addHandler(fh)

        log_message("BEGIN - LOADING AND CHECKING DATA FILES")

        data = TestData(input_file, working_dir=current_working_directory, train_model_path=model_path)

        data.check_input_file()

        log_message("END - LOADING AND CHECKING DATA FILES")

        log_message("BEGIN - CREATING TFRECORDS FILES")

        target_tfrecords_file_path = os.path.join(os.path.abspath(current_working_directory), "data.tfrecords")

        data.convert_to_tfrecords(input_file, target_tfrecords_file_path)

        log_message("END - CREATING TFRECORDS FILES")

        logging.info("{} BEGIN - APPLYING MODEL {}".format("=" * 10, "=" * 36))

        apply_model(current_working_directory, model_path, data)

        log_message("END - APPLYING MODEL")
