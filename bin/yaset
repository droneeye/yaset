#!/usr/bin/env python
import argparse
import configparser
import importlib
import logging
import os
import shutil
import sys
import time

from yaset.data.reader import TrainData, TestData
from yaset.nn.train import train_model, apply_model
from yaset.tools import ensure_dir


def get_training_parameters(configuration):
    """
    Fetch training parameters from a configparser object
    :param configuration: configparser object
    :return: a dictionary of parameters
    """

    parameters = dict()

    parameters["max_iterations"] = int(configuration["training"]["max_iterations"])
    parameters["patience"] = int(configuration["training"]["patience"])
    parameters["hidden_layer_size"] = int(configuration["training"]["hidden_layer_size"])
    parameters["cpu_cores"] = int(configuration["training"]["cpu_cores"])
    parameters["batch_size"] = int(configuration["training"]["batch_size"])
    parameters["dropout_rate"] = int(configuration["training"]["batch_size"])
    parameters["char_hidden_layer_size"] = int(configuration["training"]["char_hidden_layer_size"])
    parameters["use_char_embeddings"] = parsed_configuration.getboolean("training", "use_char_embeddings")
    parameters["trainable_word_embeddings"] = parsed_configuration.get("training", "trainable_word_embeddings")

    return parameters


def parse_feature_columns(value):

    raw_parts = value.split(",")
    raw_parts = [item.strip(" ") for item in raw_parts]

    final_list = [int(item) for item in raw_parts]

    return final_list


def log_message(message):

    logging.info("{} {} {}".format(
        "=" * 10,
        message,
        "=" * (70 - len(message) - 2 - 10)
    ))


if __name__ == "__main__":

    parser = argparse.ArgumentParser()

    parser.add_argument("--debug", action='store_true')

    subparsers = parser.add_subparsers(title="Sub-commands", description="Valid sub-commands",
                                       help="Valid sub-commands", dest="subparser_name")

    # 'Learn' subparser used to learn a new model
    parser_learn = subparsers.add_parser('LEARN', help="Learn model on train data")
    parser_learn.add_argument("--config", help="Configuration file (.ini format)", dest="config", type=str,
                              required=True)

    # 'Apply' subparser used to apply a pretrained model
    parser_test = subparsers.add_parser('APPLY', help="Apply model on test data")
    parser_test.add_argument("--model_path", help="Path to the model", dest="model_path", type=str, required=True)
    parser_test.add_argument("--input_file", help="Path to the tabulated test file", dest="input_file", required=True)
    parser_test.add_argument("--working_dir", help="Path where a working directory will be created", dest="working_dir",
                             required=True)

    args = parser.parse_args()

    # Timestamp for directory naming
    timestamp = time.strftime("%Y%m%d-%H%M%S")

    # LOGGING
    # ===============================================================
    # Logging to stdout

    log = logging.getLogger('')
    log_format = logging.Formatter("%(asctime)s %(levelname)s %(message)s")

    # Setting debug level
    if args.debug:
        log.setLevel(logging.DEBUG)
    else:
        log.setLevel(logging.INFO)

    # Toning down the verbosity of gensim (in both NORMAL and DEBUG modes)
    logging.getLogger('gensim').setLevel(logging.WARNING)

    # Adding a stdout handler
    ch = logging.StreamHandler(sys.stdout)
    ch.setFormatter(log_format)
    log.addHandler(ch)

    if args.subparser_name == "LEARN":

        # Check if config file does exist
        if not os.path.isfile(os.path.abspath(args.config)):
            raise FileNotFoundError("The configuration file you specified does not exist: {}".format(
                os.path.abspath(args.config)
            ))

        # Creating a configparser parser and parsing configuration
        parsed_configuration = configparser.ConfigParser(allow_no_value=True)
        parsed_configuration.read(os.path.abspath(args.config))

        # Checking if the top working dir specified in the configuration exists
        if not os.path.isdir(os.path.abspath(parsed_configuration["data"]["top_working_dir"])):
            raise NotADirectoryError("The top working directory you specified does not exist: {}".format(
                os.path.abspath(parsed_configuration["data"]["top_working_dir"])
            ))

        # Creating the current working directory based on the top working directory
        current_working_directory = os.path.join(
            os.path.abspath(parsed_configuration["data"]["top_working_dir"]),
            "yaset-learn-{}".format(timestamp)
        )
        ensure_dir(current_working_directory)

        # Setting up a log file and adding a new handler to the logger
        log_file = os.path.join(current_working_directory, "{}.log".format(
            "yaset-learn-{}".format(timestamp)
        ))

        fh = logging.FileHandler(log_file, encoding="UTF-8")
        fh.setFormatter(log_format)
        log.addHandler(fh)

        # Setting some variables
        train_file_path = os.path.abspath(parsed_configuration["data"]["train_file_path"])

        # -----------------------------------------------------------
        # DEV FILE

        dev_file_path = None
        dev_ratio = None
        dev_random_seed = None

        # If there is a 'dev' file provided, check if the file does exist
        if parsed_configuration.getboolean("data", "use_dev_file"):
            dev_file_path = os.path.abspath(parsed_configuration["data"]["dev_file_path"])

            if not os.path.isfile(dev_file_path):
                raise FileNotFoundError("The 'dev' file you specified doesn't exist: {}".format(
                    dev_file_path
                ))

        # If no 'dev' file is provided, fetch the train/dev ratio and check is it is between 0 and 1
        if not dev_file_path:
            dev_ratio = float(parsed_configuration["data"]["dev_ratio"])

            if dev_ratio <= 0 or dev_ratio >= 1:
                raise Exception("The 'dev' ratio must be between 0 and 1 (current ratio: {})".format(dev_ratio))

        # -----------------------------------------------------------
        # FEATURES

        feature_columns = list()

        if parsed_configuration.getboolean("data", "use_features"):
            feature_columns = parse_feature_columns(parsed_configuration["data"]["feature_columns"])

        # -----------------------------------------------------------
        # LOADING AND CHECKING DATA FILES

        log_message("BEGIN - LOADING AND CHECKING DATA FILES")

        # Creating data object
        data = TrainData(train_file_path, dev_data_file=dev_file_path, working_dir=current_working_directory,
                         dev_ratio=dev_ratio, feature_columns=feature_columns)

        # Checking file format
        data.check_input_files()

        log_message("END - LOADING AND CHECKING DATA FILES")

        # -----------------------------------------------------------
        # EMBEDDING LOADING AND PROCESSING

        log_message("BEGIN - EMBEDDING LOADING AND PREPROCESSING")

        embedding_model_type = parsed_configuration["data"]["embedding_model_type"]

        if embedding_model_type is not "random":
            # Case where the model type is not random

            logging.info("Model type: {}".format(embedding_model_type))

            # Checking if the embedding model path does exist
            embedding_file_path = os.path.abspath(parsed_configuration["data"]["embedding_model_path"])

            if not os.path.isfile(embedding_file_path):
                raise FileNotFoundError("The embedding file you specified doesn't exist: {}".format(
                    embedding_file_path
                ))

            logging.info("File path: {}".format(embedding_file_path))

            embedding_map_unknown = parsed_configuration.getboolean("data", "map_unknown_token")
            embedding_unknown_item_id = None
            if embedding_map_unknown:
                embedding_unknown_item_id = parsed_configuration.get("data", "unknown_token_id")

            # Dynamic loading of embedding module. Allow to write custom modules for specific model formats.
            logging.info("Creating embedding object")
            embedding_module = importlib.import_module("yaset.embed.{}".format(embedding_model_type))
            embedding_class = getattr(embedding_module, "{}Embeddings".format(embedding_model_type.title()))
            embedding_object = embedding_class(embedding_file_path, embedding_unknown_item_id)

            # Loading embedding matrix into embedding object
            logging.info("Loading matrix")
            embedding_object.load_embedding()

            if not embedding_map_unknown:
                # Building unknown token vector based on embedding matrix
                logging.info("Building unknown token vector")
                embedding_unknown_item_id = embedding_object.build_unknown_token()
            else:
                logging.info("Unknown token vector already exists, skipping building new one (id={})".format(
                    embedding_unknown_item_id
                ))

        else:
            # Random embedding will be supported in a later release
            raise Exception("Random embeddings are not supported yet")

        log_message("END - EMBEDDING LOADING AND PREPROCESSING")

        log_message("BEGIN - CREATING TFRECORDS FILES")

        if parsed_configuration.getboolean("data", "dev_use_random_seed") and \
                not parsed_configuration.getboolean("data", "use_dev_file"):
            dev_random_seed = parsed_configuration.getint("data", "dev_random_seed")

        data.create_tfrecords_files(embedding_object, random_seed=dev_random_seed)

        log_message("END - CREATING TFRECORDS FILES")

        log_message("BEGIN - LEARNING MODEL")

        train_config = get_training_parameters(parsed_configuration)

        logging.debug("Current training parameters")
        for k, v in train_config.items():
            logging.debug("* {} = {}".format(k, v))

        train_model(current_working_directory, embedding_object, data, train_config)

        target_model_configuration_path = os.path.join(os.path.abspath(current_working_directory), "config.ini")
        shutil.copy(os.path.abspath(args.config), target_model_configuration_path)

        log_message("END - LEARNING MODEL")

    elif args.subparser_name == "APPLY":

        model_path = os.path.abspath(args.model_path)
        input_file = os.path.abspath(args.input_file)
        working_dir = os.path.abspath(args.working_dir)

        if not os.path.isdir(model_path):
            raise NotADirectoryError("The model path you specified does not exist: {}".format(model_path))

        if not os.path.isfile(input_file):
            raise FileNotFoundError("The input file you specified does not exist: {}".format(input_file))

        if not os.path.isdir(working_dir):
            raise NotADirectoryError("The working directory you specified does not exist: {}".format(working_dir))

        current_working_directory = os.path.join(working_dir, "yaset-apply-{}".format(timestamp))
        ensure_dir(current_working_directory)

        log_message("BEGIN - LOADING AND CHECKING DATA FILES")

        data = TestData(input_file, working_dir=current_working_directory, train_model_path=model_path)

        data.check_input_file()

        log_message("END - LOADING AND CHECKING DATA FILES")

        log_message("BEGIN - CREATING TFRECORDS FILES")

        target_tfrecords_file_path = os.path.join(os.path.abspath(current_working_directory), "data.tfrecords")

        data.convert_to_tfrecords(input_file, target_tfrecords_file_path)

        log_message("END - CREATING TFRECORDS FILES")

        logging.info("{} BEGIN - APPLYING MODEL {}".format("=" * 10, "=" * 36))

        apply_model(current_working_directory, model_path, data)

        log_message("END - APPLYING MODEL")
