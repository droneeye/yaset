[data]

# ===================================================================
# DATA FILES
# ===================================================================

# -------------------------------------------------------------------
# TRAIN

# Path to the train file
train_file_path = /path/to/data/train.tab

# -------------------------------------------------------------------
# DEV

# Do you want to specify a dev file ('true' or 'false')?
dev_file_use = true

# If you want to use a dev file, specify the path to file.
# This will be ignored if the value of the previous parameter is 'false'.
dev_file_path = /path/to/data/dev.tab

# If you do not want to use a dev file, specify the percentage of the training instances that should be kept as
# development instances (float between 0 and 1)
dev_random_ratio = 0.2

# Do you want to use a random seed for the train/dev split (in the case you do not want to specify a dev file)
dev_random_seed_use = true

# If you want to use a random seed, specify the seed (integer).
# This will be ignored if the value of the previous parameter is 'false'.
dev_random_seed_value = 42

# -------------------------------------------------------------------
# DATA PREPROCESSING

# Do you want to lowercase the tokens (first column)? This will not affect character emebddings computation (if used).
preproc_lower_input = true

# Do you want to replace digits with "0" in the tokens?
preproc_replace_digits = true

# -------------------------------------------------------------------
# FEATURES

# NOT YET FUNCTIONAL

# Do you want to use other features besides the tokens?
feature_use = false

# If you want to use other features, please specified the column IDs (0-indexed)
feature_columns = 1,2,3

# ===================================================================
# EMBEDDING MODEL
# ===================================================================

# Model type (only 'gensim' or 'word2vec' models are accepted)
embedding_model_type = gensim

# Path to the pretrained model
embedding_model_path = /path/to/data/gensim-model.pkl


# Choose the strategy for Out-Of-Vocabulary tokens
# ------------------------------------------------
#
# 1. 'map': you have a vector for OOV tokens in the embedding file you provided. Set 'embedding_oov_strategy' to 'map'
#   and specify the OOV vector ID ('embedding_oov_map_token_id')
#
# 2. 'replace': following Lample et al. (2016), an OOV vector will be randomly generated and trained by randomly
#   replacing singletons in the training instances with the special token 'unknown'. You can adjust the replacement
#   rate by changing the value of the 'embedding_oov_replace_rate' parameter

embedding_oov_strategy = replace
embedding_oov_map_token_id = #UNK#

embedding_oov_replace_rate = 0.5

# ===================================================================
# WORKING DIR
# ===================================================================

# Path where a timestamped working directory will be created
working_dir = /path/to/working/dir/

[training]

# Deep learning model ('bilstm-char-crf')
model_type = bilstm-char-crf

# -------------------------------------------------------------------
# ITERATIONS AND EARLY STOPPING

# Maximum number of iterations for training
max_iterations = 100

# Maximum number of iteration after the best score has been obtained
patience = 20

# -------------------------------------------------------------------
# METRICS

# Metric used for dev performance measurement ('accuracy' or 'conll')
dev_metric = conll

# -------------------------------------------------------------------
# WORD EMBEDDINGS

# Do you want to continue the training of the word embeddings?
trainable_word_embeddings = false

# -------------------------------------------------------------------
# MISC

# Number of CPU cores to use during training (upper-bound)
cpu_cores = 4

# Mini-btach size used during training
batch_size = 256

# If using GPU, do you want to store embedding matrices on GPU memory?
store_matrices_on_gpu = true

# Bucketize the input sequences for faster training
bucket_use = true

# -------------------------------------------------------------------
# OPTIMIZATION ALGORITHM

# Optimization algorithm ('adam' or 'sgd')
opt_algo = sgd

# Initial learning rate
opt_lr = 0.01

# Use gradient clipping?
opt_gc_use = true
# Specify the gradient clipping type ('clip_by_norm' or 'clip_by_value')
opt_gc_type = clip_by_norm
# Gradient clipping value
opt_gs_val = 5.0

# Use exponential decay?
opt_decay_use = true
# Exponential decay rate
opt_decay_rate = 0.9
# Decay every 'n' iterations
opt_decay_iteration = 1

[bilstm-char-crf]

# -------------------------------------------------------------------
# MAIN BiLSTM ARCHITECTURE

# Number of units in the main BiLSTM
hidden_layer_size = 256

# Dropout rate applied during training on input embeddings
dropout_rate = 0.5

# -------------------------------------------------------------------
# CHAR BiLSTM ARCHITECTURE

# Use character embeddings in the model. These embeddings are learned during network training.
use_char_embeddings = true

# Number of units in the character BiLSTM
char_hidden_layer_size = 25

# Character emebdding size
char_embedding_size = 8
